{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funnynet\n",
    "\n",
    "## A neural network that makes jokes\n",
    "\n",
    "Special thanks to taivop for providing the [dataset](https://github.com/taivop/joke-dataset).\n",
    "\n",
    "This notebook is heavily inspired by [fastai NLP work](https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "EOJ = 'xeoj'  # end of joke tag\n",
    "\n",
    "PATH=Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data/val'), PosixPath('data/.ipynb_checkpoints'), PosixPath('data/nietzsche'), PosixPath('data/trn'), PosixPath('data/stupidstuff.json'), PosixPath('data/reddit_jokes.json')]\n"
     ]
    }
   ],
   "source": [
    "files = list(PATH.iterdir())\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in files:\n",
    "    if \"eddit\" in str(fname):\n",
    "        reddit_dataset = str(fname)\n",
    "    if \"upid\" in str(fname):\n",
    "        stupid_dataset = str(fname)\n",
    "reddit_jokes = json.load(open(reddit_dataset))\n",
    "stupid_jokes = json.load(open(stupid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discard all the jokes that have 0 score, as they aren't that helpful for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_jokes = [joke for joke in reddit_jokes if joke['score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172.94791416025024, 48526)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [joke['score'] for joke in rated_jokes]\n",
    "np.mean(scores),np.max(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many jokes have really high scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hate how you cant even say black paint anymore Now I have to say \"Leroy can you please paint the fence?\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_body = [joke['title']+' '+joke['body'] for joke in rated_jokes]\n",
    "title_body[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horrible, but formatted correctly. Now, let's combine all the jokes into one long string, using the `EOJ` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe6468cd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_jokes = []\n",
    "for i in range(len(reddit_jokes)):\n",
    "    r_joke = reddit_jokes[i]\n",
    "    #|print(r_joke)\n",
    "    r_joke['rating']=round(math.log(r_joke['score']+random.randrange(1,10))/math.log(10)*5/2, 2)\n",
    "    if r_joke['rating']>5:\n",
    "        r_joke['rating']=5\n",
    "    del r_joke['score'] \n",
    "    r_joke['body'] = r_joke['title']+\" \"+r_joke['body']\n",
    "    del r_joke['title']\n",
    "for s_joke in stupid_jokes:\n",
    "    del s_joke['category']\n",
    "\n",
    "combined = [joke['rating'] for joke in reddit_jokes]\n",
    "combined = combined + [joke['rating'] for joke in stupid_jokes]\n",
    "plt.hist(combined,bins=5);\n",
    "\n",
    "combined_jokes = reddit_jokes + stupid_jokes\n",
    "\n",
    "title_body = [joke['body']+' ' for joke in combined_jokes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for joke in title_body:\n",
    "    text = text + ' ' + joke + ' ' + EOJ + ' '\n",
    "    if len(text) > 800000: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 125\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x00', '\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '\\x9d', '\\xa0', '¢', '£', '°', '´', 'è', 'é', 'ñ', 'ó', 'μ', 'π', '–', '—', '‘', '’', '“', '”', '•', '…', '\\u2028', '€', '√', '∫', '\\ufeff', '🇩', '🇰', '😂', '😨', '🤣']\n"
     ]
    }
   ],
   "source": [
    "chars.insert(0, \"\\0\")\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 44, 3, 75, 68, 87, 72, 3, 75, 82]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I hate how you cant even say black paint anymore Now I have to say \"L'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Three character model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "embeddings_sz = 42 # size of embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries require some setup, try the pip install git+https.github.com/... trick\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "\n",
    "from fastai.column_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-02016477e9cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColumnarModelData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ThreeCharRNN(vocab_size, embeddings_sz).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a bigger RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_len=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_in_jokes = [[idx[i+j] for i in range(rnn_len)] for j in range(len(idx)-rnn_len)]\n",
    "\n",
    "char_input = []\n",
    "for j in range(len(idx)-rnn_len):\n",
    "    tmp = []\n",
    "    for i in range(rnn_len):\n",
    "        tmp.append(idx[i+j])\n",
    "    char_input.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_output = []\n",
    "for j in range(len(idx)-rnn_len):\n",
    "    char_output.append(idx[j+rnn_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.stack(char_input, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(char_output)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:rnn_len,:rnn_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[:rnn_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(idx)-rnn_len-1)\n",
    "# val_idx.shape\n",
    "model_data = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_sz):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, embeddings_sz)\n",
    "        self.l_in = nn.Linear(embeddings_sz, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *rnn_len):\n",
    "#         pdb.set_trace()\n",
    "        bs = rnn_len[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden).cuda())\n",
    "        for c in rnn_len:\n",
    "            inp = F.relu(self.l_in(self.e(c)))\n",
    "            h = F.tanh(self.l_hidden(h+inp))\n",
    "            \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharLoopModel(vocab_size, embeddings_sz).cuda()\n",
    "opt = optim.Adam(model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, model_data, epochs=1, opt=opt, crit=F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, model_data, epochs=1, opt=opt, crit=F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = model(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next(' a blond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('into a b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_n('into a b', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopConcatModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_sz):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, embeddings_sz)\n",
    "        self.l_in = nn.Linear(embeddings_sz+n_hidden, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *rnn_len):\n",
    "        bs = rnn_len[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden).cuda())\n",
    "        for c in rnn_len:\n",
    "            inp = torch.cat((h, self.e(c)), 1)\n",
    "            inp = F.relu(self.l_in(inp))\n",
    "            h = F.tanh(self.l_hidden(inp))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopConcatModel(vocab_size, embeddings_sz).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('wome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('beca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('char')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Multiple Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_sz):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, embeddings_sz)\n",
    "        self.rnn = nn.RNN(embeddings_sz, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *rnn_len):\n",
    "        #print(\"rnn_len: \"+str(rnn_len))\n",
    "        bs = rnn_len[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(rnn_len))\n",
    "        outp,h = self.rnn(inp, h)\n",
    "        \n",
    "        return F.log_softmax(self.l_out(outp[-1]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharRnn(vocab_size, embeddings_sz).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = m.e(V(torch.stack(xs)))\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = V(torch.zeros(1, 512,n_hidden))\n",
    "outp, hn = m.rnn(t, ht)\n",
    "outp.size(), hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = m(*V(xs)); t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 2, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_n('for thos', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+j] for i in range(rnn_len)] for j in range(0, len(idx)-rnn_len-1, rnn_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+j] for i in range(rnn_len)] for j in range(1, len(idx)-rnn_len, rnn_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.stack(c_in_dat)\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:rnn_len,:rnn_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.stack(c_out_dat)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[:rnn_len,:rnn_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(xs)-rnn_len-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_sz):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, embeddings_sz)\n",
    "        self.rnn = nn.RNN(embeddings_sz, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *rnn_len):\n",
    "        bs = rnn_len[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(rnn_len))\n",
    "        outp,h = self.rnn(inp, h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, embeddings_sz).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xst,yt = next(it)\n",
    "#t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    return F.nll_loss(inp.view(-1,nh), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, embeddings_sz).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding State: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=64; bptt=8; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)\n",
    "\n",
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "Now we will try an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "from fastai import sgdr\n",
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(xs)-rnn_len-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_sz, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, embeddings_sz)\n",
    "        self.rnn = nn.LSTM(embeddings_sz, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, rnn_len):\n",
    "        bs = rnn_len[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(rnn_len), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(md.nt, embeddings_sz, 512, 2).cuda()\n",
    "#lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
