{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funnynet\n",
    "\n",
    "## A neural network that makes jokes\n",
    "\n",
    "Special thanks to taivop for providing the [dataset](https://github.com/taivop/joke-dataset).\n",
    "\n",
    "This notebook is heavily inspired by [fastai NLP work](https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "EOJ = 'xeoj'  # end of joke tag\n",
    "\n",
    "PATH=Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data/reddit_jokes.json'), PosixPath('data/stupidstuff.json')]\n"
     ]
    }
   ],
   "source": [
    "files = list(PATH.iterdir())\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in files:\n",
    "    if \"eddit\" in str(fname):\n",
    "        reddit_dataset = str(fname)\n",
    "    if \"upid\" in str(fname):\n",
    "        stupid_dataset = str(fname)\n",
    "reddit_jokes = json.load(open(reddit_dataset))\n",
    "stupid_jokes = json.load(open(stupid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194553"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': 'Now I have to say \"Leroy can you please paint the fence?\"',\n",
       " 'id': '5tz52q',\n",
       " 'score': 1,\n",
       " 'title': 'I hate how you cant even say black paint anymore'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_jokes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discard all the jokes that have 0 score, as they aren't that helpful for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_jokes = [joke for joke in reddit_jokes if joke['score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132992"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rated_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172.94791416025024, 48526)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [joke['score'] for joke in rated_jokes]\n",
    "np.mean(scores),np.max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_scores = [score for score in scores]\n",
    "# plt.xscale('log', nonposx='clip')\n",
    "# plt.ylim(ymax=200000)\n",
    "# plt.axes.set_ylim([0,200000])\n",
    "# plt.hist(low_scores, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hate how you cant even say black paint anymore Now I have to say \"Leroy can you please paint the fence?\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_body = [joke['title']+' '+joke['body'] for joke in rated_jokes]\n",
    "title_body[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horrible, but formatted correctly. Now, let's combine all the jokes into one long string, using the `EOJ` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for joke in title_body:\n",
    "    text = text + ' ' + joke + ' ' + EOJ + ' '\n",
    "    if len(text) > 2700000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2700071"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)\n",
    "# print(text[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 164\n",
      "['\\x00', '\\t', '\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '\\x9d', '\\xa0', '¬¢', '¬£', '¬´', '¬∞', '¬¥', '¬ª', '√ë', '√ì', '√ó', '√†', '√§', '√®', '√©', '√´', '√≠', '√±', '√≥', ' ñ', 'Õú', 'Õ°', 'Œ£', 'Œº', 'œÄ', '\\u2009', '\\u200b', '\\u200f', '‚Äì', '‚Äî', '‚Äò', '‚Äô', '‚Äú', '‚Äù', '‚Ä¢', '‚Ä¶', '\\u2028', '‚Ä≤', '‚Ä≥', '‚ÄΩ', '‚Ç¨', '‚àÜ', '‚àö', '‚à´', '‚òù', '‚ô´', '‚ôª', '‰∏ª', '‰∫∫', 'Êñã', 'Êûó', 'ÊµÆ', 'ÁôΩ', 'Á¨ë', '\\ufeff', 'üá©', 'üá∞', 'üòÅ', 'üòÇ', 'üòà', 'üòä', 'üòé', 'üòú', 'üò®', 'üò≥', 'üôÑ', 'ü§£']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)\n",
    "chars.insert(0, \"\\0\")\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears there are Emojis in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 45, 4, 76, 69, 88, 73, 4, 76, 83]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I hate how you cant even say black paint anymore Now I have to say \"L'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup, creating .txt files to train LSTM on\n",
    "\n",
    "Partitioning the dataset and writing it to /data/trn/trn.txt and /data/val/val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DANIEL_PATH = Path('data/')\n",
    "(DANIEL_PATH/'val').mkdir(exist_ok=True)\n",
    "(DANIEL_PATH/'trn').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/reddit_jokes.json'),\n",
       " PosixPath('data/val'),\n",
       " PosixPath('data/stupidstuff.json'),\n",
       " PosixPath('data/trn')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DANIEL_PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = open(TRN+\"trn.txt\",\"wb\")\n",
    "trn.write(text[:len(text)*4//5].encode('utf-8'))#str(idx[0:int(len(idx)*2/3)]))\n",
    "trn.close()\n",
    "\n",
    "val = open(VAL+\"val.txt\",\"wb\")\n",
    "val.write(text[len(text)*4//5:].encode('utf-8'))#str(idx[int(len(idx)*2/3):len(idx)-1]))\n",
    "val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_jokes.json  stupidstuff.json  \u001b[0m\u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "PATH='data/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this next part assumes we have already partitioned the data into trn/trn.txt and val/val.txt for training and validation sets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: <torchtext.data.field.Field object at 0x7f4cfc486780>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4178, 92, 1, 2139781)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "print(\"TEXT: \"+str(TEXT))\n",
    "bs=64; bptt=8; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "Now we will try an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "from fastai import sgdr\n",
    "n_hidden = 256\n",
    "embeddings_sz = 42 # size of embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_sz, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, embeddings_sz)\n",
    "        self.rnn = nn.LSTM(embeddings_sz, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, rnn_len):\n",
    "        bs = rnn_len[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(rnn_len), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(len(chars)+1,embeddings_sz, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e016a36aa02445aaa942d749f18a3203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.714574   1.664388  \n",
      "    1      1.646911   1.612117                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.61212])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He trains for 2^6 epochs... I'm not waiting that long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a77a1a94f2496e8837b72dee74a670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.521971   1.465887  \n",
      "    1      1.563488   1.512391                                \n",
      "    2      1.459124   1.416796                                \n",
      "    3      1.582633   1.527102                                \n",
      "    4      1.533194   1.487429                                \n",
      "    5      1.457572   1.416012                                \n",
      "    6      1.40332    1.370478                                \n",
      "    7      1.558284   1.513639                                \n",
      "    8      1.561605   1.522661                                \n",
      "    9      1.538751   1.490585                                \n",
      "    10     1.501419   1.458146                                \n",
      "    11     1.469132   1.422798                                \n",
      "    12     1.439545   1.389487                                \n",
      "    13     1.387424   1.354124                                \n",
      "    14     1.366881   1.331787                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.33179])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e824fddf253488fb7bba201801aecd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.354918   1.3269    \n",
      "    1      1.352675   1.323687                                \n",
      "    2      1.347158   1.320351                                \n",
      "    3      1.350975   1.319187                                \n",
      "    4      1.348225   1.314779                                \n",
      "    5      1.346236   1.3113                                  \n",
      "    6      1.33694    1.309875                                \n",
      "    7      1.339971   1.311275                                \n",
      "    8      1.342709   1.307846                                \n",
      "    9      1.331497   1.304511                                \n",
      "    10     1.325322   1.300965                                \n",
      "    11     1.322134   1.298711                                \n",
      "    12     1.323038   1.296354                                \n",
      "    13     1.320237   1.295142                                \n",
      "    14     1.326954   1.294583                                \n",
      "    15     1.314131   1.298129                                \n",
      "    16     1.319001   1.296318                                \n",
      "    17     1.315008   1.294002                                \n",
      "    18     1.311213   1.291836                                \n",
      "    19     1.310596   1.289695                                \n",
      "    20     1.303473   1.287294                                \n",
      "    21     1.300431   1.285732                                \n",
      "    22     1.30465    1.283517                                \n",
      "    23     1.295316   1.282213                                \n",
      "    24     1.292651   1.280719                                \n",
      "    25     1.295381   1.279073                                \n",
      "    26     1.292623   1.277829                                \n",
      "    27     1.289292   1.277215                                \n",
      "    28     1.291985   1.276487                                \n",
      "    29     1.288552   1.276041                                \n",
      "    30     1.286933   1.275931                                \n",
      "    31     1.285903   1.275801                                \n",
      "    32     1.293182   1.281185                                \n",
      "    33     1.290028   1.28063                                 \n",
      "    34     1.284986   1.278297                                \n",
      "    35     1.288531   1.277727                                \n",
      "    36     1.287715   1.276431                                \n",
      "    37     1.28656    1.275084                                \n",
      "    38     1.287518   1.274179                                \n",
      "    39     1.277904   1.272848                                \n",
      "    40     1.284334   1.271471                                \n",
      "    41     1.278421   1.270873                                \n",
      "    42     1.281607   1.269025                                \n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2731/4178 [00:28<00:15, 94.56it/s, loss=1.27]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    45     1.271087   1.265574                                \n",
      "    46     1.266298   1.264458                                \n",
      "    47     1.264962   1.263703                                \n",
      "    48     1.266599   1.263053                                \n",
      "    49     1.270917   1.261857                                \n",
      "    50     1.265776   1.261233                                \n",
      "    51     1.263597   1.260589                                \n",
      "    52     1.260333   1.259923                                \n",
      "    53     1.266673   1.258903                                \n",
      "    56     1.259735   1.25756                                 \n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1998/4178 [00:21<00:23, 94.08it/s, loss=1.24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    58     1.261125   1.256967                                \n",
      " 18%|‚ñà‚ñä        | 747/4178 [00:08<00:36, 92.91it/s, loss=1.26]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3791/4178 [00:42<00:04, 89.62it/s, loss=1.26]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    61     1.258265   1.256377                                \n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2613/4178 [00:29<00:17, 90.01it/s, loss=1.24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('into a b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('a ma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('blon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "into a bar  cut) xeoj  differencerokeigato cats? coffee is my best said, \"jesus clear, i find my one‚Äôs grandma that goes togetherics.little perfects murdering, he throwing hubby? there is dear instruction you pape of weaking a city? *nine timestoralie. xeoj  what wat the parastan't walked into a bathroom and asked what nates the world.\"the side, plants from shopped out the couple of street. the greve. he \n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('into a b', 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_next_n('blon', 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you think how i fuck us \" after god doing jack hole trump are hitting us. say, and to look down!\" xeoj  what they're fadio ckas feel again and my friend for a 25, 4,52949 silent, the opportunity enough to figure of any fute. xeoj  people because for jesus. xeoj  i want to get him back. the third has walky from her man accident, none that was in his calm.last woman looking at a nip.\" the man is begi\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('what do y',400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n_jokes(prompt, n):\n",
    "    jokes = []\n",
    "    c = ''\n",
    "    for j in range(n):\n",
    "        inp = prompt\n",
    "        res = prompt\n",
    "        while res[-5:-1]!=\"xeoj\":\n",
    "            c = get_next(inp)\n",
    "            res += c\n",
    "            inp = inp[1:]+c\n",
    "        jokes.append(res)\n",
    "    return jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-1ffca366b39f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mjoke\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_next_n_jokes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you hear about the university book store worker\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-1a74f7d261e5>\u001b[0m in \u001b[0;36mget_next_n_jokes\u001b[0;34m(prompt, n)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"xeoj\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-4647cae1d09a>\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for joke in get_next_n_jokes(\"you hear about the university book store worker\",4):\n",
    "    print(joke)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hate how you can't stop for 7 years. he flings his dibty-out to sitting at a bell and asked the faster \"\"we started carrying and utteringal at a heary at this first \"could you refresh?\" the man comes back to his bought and scared as it's only day, \"i'm walking on by because. the battle runs farted to the entire lives of arm off a body where he refugeed to be from his chair woman as heaven, but she sat up was water.one of it and he finds the ownerhip showing up. this is an old man cated i but start coasination xeoj \n",
      "\n",
      "\n",
      "i hate how you can't st.  \"hey, so no, are an expensive breaked the temperature to left, he makes the lead, and the swimming rules in a back of holds. and the inside to great as and old : i just make it off her healfuses. in the wines were logged by a good shop-across and thought more really englishman, thought he heard enough trishung. she giving the hope to be fruit as she was a new debuted pretty drinking and puts it.but they can get it on easy! xeoj \n",
      "\n",
      "\n",
      "i hate how you can't see swet president and asked, \"what does it?‚Äù xeoj \n",
      "\n",
      "\n",
      "i hate how you can't stupid dick?\"i xeoj \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for joke in get_next_n_jokes(\"i hate how you can't s\",4):\n",
    "    print(joke)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why women need legs. xeoj \n",
      "\n",
      "\n",
      "why women need legse fishing one of his back and starts sboft ingurily.from bay. \"you're everyone were cartain. xeoj \n",
      "\n",
      "\n",
      "why women need legsy arms watting the attraction of the guy gets not try. **apain skin on eyem! xeoj \n",
      "\n",
      "\n",
      "why women need legs for a few million loop, and this were holding its time. replied, ‚Äú'jae, so it's jewish tases xeoj \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for joke in get_next_n_jokes('why women need legs',4):\n",
    "    print(joke)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
