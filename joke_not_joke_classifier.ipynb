{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import math, random\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import word_category_counter\n",
    "\n",
    "#Get our data\n",
    "#Label data based on rating\n",
    "#Extract features\"\n",
    "#split between train, dev, and test\n",
    "\n",
    "#Choose classifier \n",
    "#Train classifier (on train data)\n",
    "\n",
    "#Test classifier (on dev data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "[PosixPath('data/0OYkPK'), PosixPath('data/7R59b1'), PosixPath('data/data'), PosixPath('data/enwiki-20180320-abstract.xml'), PosixPath('data/enwiki-20180320-abstract.xml.gz'), PosixPath('data/fastai'), PosixPath('data/LIWC2007.dic'), PosixPath('data/out'), PosixPath('data/reddit_jokes.json'), PosixPath('data/ROCStories_winter2017 - ROCStories_winter2017.csv'), PosixPath('data/stupidstuff.json'), PosixPath('data/val')]\n",
      "data/enwiki-20180320-abstract.xml\n",
      "66316747\n",
      "3407212\n",
      "test\n",
      "storyid,storytitle,sentence1,sentence2,sentence3,sentence4,sentence5\n",
      "\n",
      "The orange fell from the tree. It hit a girl on the head.\n"
     ]
    }
   ],
   "source": [
    "#Get wiki (non-joke) data\n",
    "print(\"hello\")\n",
    "PATH = Path('data')\n",
    "files = list(PATH.iterdir())\n",
    "print(files)\n",
    "for fname in files:\n",
    "    if \"enwiki\" in str(fname) and \"gz\" not in str(fname):\n",
    "        wiki_dataset = str(fname)\n",
    "    if \"ROC\" in str(fname):\n",
    "        story_dataset = str(fname)\n",
    "print(wiki_dataset)\n",
    "num_lines = sum(1 for line in open(wiki_dataset))\n",
    "print(num_lines)\n",
    "wiki_file = open(wiki_dataset,\"rb\")#.decode(\"unicode\")\n",
    "abstracts = []\n",
    "for i in range(num_lines):\n",
    "    #print(i)\n",
    "    nextline = wiki_file.readline().decode(\"utf-8\")\n",
    "    if \"bstract\" in nextline and \"|\" not in nextline and \"(can)|(may) refer to\" and nextline != \"__NOTOC__\":\n",
    "        nextline = re.sub(\"</?abstract>\",\"\",nextline)\n",
    "        nextline = re.sub(\"}}\",\"\",nextline)\n",
    "        #print(nextline)\n",
    "        abstracts.append(nextline)\n",
    "    #if \"he\" in nextline:\n",
    "    #    break\n",
    "print(len(abstracts))        \n",
    "print(\"test\")\n",
    "story_file = open(story_dataset)\n",
    "num_lines = sum(1 for line in open(story_dataset))\n",
    "print(story_file.readline())\n",
    "stories = []\n",
    "for i in range(1,num_lines):\n",
    "    nextline = story_file.readline()\n",
    "    cols = nextline.split(\",\")\n",
    "    story = \" \".join(cols[2:random.randrange(3,7)])\n",
    "    stories.append(story)\n",
    "print(story)\n",
    "notjokes = stories #abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get joke data\n",
    "\n",
    "#standard preprocessing\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "EOJ = 'xeoj'  # end of joke tag\n",
    "\n",
    "#get jokes:\n",
    "PATH=Path('data')\n",
    "\n",
    "files = list(PATH.iterdir())\n",
    "\n",
    "for fname in files:\n",
    "    if \"eddit\" in str(fname):\n",
    "        reddit_dataset = str(fname)\n",
    "    if \"upid\" in str(fname):\n",
    "        stupid_dataset = str(fname)\n",
    "reddit_jokes = json.load(open(reddit_dataset))\n",
    "stupid_jokes = json.load(open(stupid_dataset))\n",
    "\n",
    "#discard reddit jokes with 0 score:\n",
    "rated_jokes = [joke for joke in reddit_jokes if joke['score'] > 0]\n",
    "\n",
    "#regularize to match stupid_jokes:\n",
    "title_body = [joke['title']+' '+joke['body'] for joke in rated_jokes]\n",
    "\n",
    "all_jokes = []\n",
    "for i in range(len(reddit_jokes)):\n",
    "    r_joke = reddit_jokes[i]\n",
    "    #|print(r_joke)\n",
    "    r_joke['rating']=round(math.log(r_joke['score']+random.randrange(1,10))/math.log(10)*5/2, 2)\n",
    "    if r_joke['rating']>5:\n",
    "        r_joke['rating']=5\n",
    "    del r_joke['score'] \n",
    "    r_joke['body'] = r_joke['title']+\" \"+r_joke['body']\n",
    "    del r_joke['title']\n",
    "for s_joke in stupid_jokes:\n",
    "    del s_joke['category']\n",
    "\n",
    "#combine joke sets:\n",
    "combined_jokes = reddit_jokes + stupid_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91652, 106674)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group into funny and notFunny sets:\n",
    "\n",
    "funny_joke_list = []\n",
    "not_funny_joke_list = []\n",
    "for joke in combined_jokes:\n",
    "    if joke[\"rating\"] >= 2.5:\n",
    "        funny_joke_list.append(joke)\n",
    "    else:\n",
    "        not_funny_joke_list.append(joke)\n",
    "        \n",
    "(len(funny_joke_list), len(not_funny_joke_list) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unigrams and bigrams:\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def normalize(text):\n",
    "    tokenized_text = []\n",
    "    tags = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        intermediate = [word for word in nltk.word_tokenize(sent) if (word not in stopwords) and re.search(r\"\\w\", word)]\n",
    "        for word, pos in nltk.pos_tag(intermediate):\n",
    "            tokenized_text.append(word.lower())\n",
    "            tags.append(pos)\n",
    "    return tokenized_text, tags\n",
    "\n",
    "def get_ngrams(tokens):\n",
    "    unigrams = nltk.FreqDist(tokens)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    \n",
    "    feature_vector = {}\n",
    "    for token, freq in unigrams.items():\n",
    "        feature_vector[\"UNI_%s\" %(token)] = float(freq)/unigrams.N()\n",
    "    for (token1, token2), freq in bigrams.items():\n",
    "        feature_vector[\"BI_(%s,%s)\" %(token1,token2)] = float(freq)/bigrams.N()        \n",
    "    return feature_vector\n",
    "        #\"%s ahhhhh! %s\" %(\"sdflks\", \"sdff\")\n",
    "    \n",
    "def get_pos(tags):\n",
    "    unigrams = nltk.FreqDist(tags)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tags))\n",
    "    \n",
    "    feature_vector = {}\n",
    "    for token, freq in unigrams.items():\n",
    "        feature_vector[\"UNIPOS_%s\" %(token)] = float(freq)/unigrams.N()\n",
    "    for (token1, token2), freq in bigrams.items():\n",
    "        feature_vector[\"BIPOS_(%s,%s)\" %(token1,token2)] = float(freq)/bigrams.N()        \n",
    "    return feature_vector\n",
    "\n",
    "def get_liwc_features(tokens):\n",
    "    \"\"\"\n",
    "    Adds all possible LIWC derived feature\n",
    "\n",
    "    :param words:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text = u\" \".join(tokens)\n",
    "    liwc_cat = ['Total Function Words',\n",
    "                'Total Pronouns',\n",
    "                'Personal Pronouns',\n",
    "                'First Person Singular',\n",
    "                'First Person Plural',\n",
    "                'Second Person',\n",
    "                'Third Person Singular',\n",
    "                'Third Person Plural',\n",
    "                ' Impersonal Pronouns',\n",
    "                'Articles',\n",
    "                'Common Verbs',\n",
    "                'Auxiliary Verbs',\n",
    "                'Past Tense',\n",
    "                'Present Tense',\n",
    "                'Future Tense',\n",
    "                'Adverbs',\n",
    "                'Prepositions',\n",
    "                'Conjunctions',\n",
    "                'Negations',\n",
    "                'Quantifiers',\n",
    "                'Number',\n",
    "                'Swear Words',\n",
    "                'Social Processes',\n",
    "                'Family',\n",
    "                'Friends',\n",
    "                'Humans',\n",
    "                'Affective Processes',\n",
    "                'Positive Emotion',\n",
    "                'Negative Emotion',\n",
    "                'Anxiety',\n",
    "                'Anger',\n",
    "                'Sadness',\n",
    "                'Cognitive Processes',\n",
    "                'Insight',\n",
    "                'Causation',\n",
    "                'Discrepancy',\n",
    "                'Tentative',\n",
    "                'Certainty',\n",
    "                'Inhibition',\n",
    "                'Inclusive',\n",
    "                'Exclusive',\n",
    "                'Perceptual Processes',\n",
    "                'See',\n",
    "                'Hear',\n",
    "                'Feel',\n",
    "                'Biological Processes',\n",
    "                'Body',\n",
    "                'Health',\n",
    "                'Sexual',\n",
    "                'Ingestion',\n",
    "                'Relativity',\n",
    "                'Motion',\n",
    "                'Space',\n",
    "                'Time',\n",
    "                'Work',\n",
    "                'Achievement',\n",
    "                'Leisure',\n",
    "                'Home',\n",
    "                'Money',\n",
    "                'Religion',\n",
    "                'Death',\n",
    "                'Assent',\n",
    "                'Nonfluencies',\n",
    "                'Fillers',\n",
    "                'Total first person',\n",
    "                'Total third person',\n",
    "                'Positive feelings',\n",
    "                'Optimism and energy',\n",
    "                'Communication',\n",
    "                'Other references to people',\n",
    "                'Up',\n",
    "                'Down',\n",
    "                'Occupation',\n",
    "                'School',\n",
    "                'Sports',\n",
    "                'TV',\n",
    "                'Music',\n",
    "                'Metaphysical issues',\n",
    "                'Physical states and functions',\n",
    "                'Sleeping',\n",
    "                'Grooming']\n",
    "\n",
    "    feature_vectors = {}\n",
    "    liwc_scores = word_category_counter.score_text(text)\n",
    "\n",
    "    negative_score = liwc_scores[\"Negative Emotion\"]\n",
    "    positive_score = liwc_scores[\"Positive Emotion\"]\n",
    "\n",
    "    if positive_score > negative_score:\n",
    "        feature_vectors[\"liwc:positive\"] = 1\n",
    "    else:\n",
    "        feature_vectors[\"liwc:negative\"] = 1\n",
    "\n",
    "    for cat in liwc_cat:\n",
    "        if cat in liwc_scores:\n",
    "            label = cat.lower().replace(\" \", \"_\")\n",
    "            feature_vectors[\"liwc_%s\" %label] = liwc_scores[cat]\n",
    "\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_feature_tuples = []\n",
    "set_size= 4000\n",
    "for joke in combined_jokes[:set_size]:\n",
    "    tokens, tags = normalize(joke[\"body\"])\n",
    "    joke_feature_tuples.append(({**get_ngrams(tokens), **get_pos(tags), **get_liwc_features(tokens)},\"joke\"))\n",
    "    \n",
    "notjoke_feature_tuples = []\n",
    "for abstract in notjokes[:set_size]:\n",
    "    tokens, tags = normalize(abstract)\n",
    "    notjoke_feature_tuples.append(({**get_ngrams(tokens), **get_pos(tags), **get_liwc_features(tokens)},\"not_joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'BIPOS_(JJ,NN)': 0.05263157894736842,\n",
       "  'BIPOS_(MD,VBG)': 0.05263157894736842,\n",
       "  'BIPOS_(NN,NN)': 0.15789473684210525,\n",
       "  'BIPOS_(NN,PRP)': 0.05263157894736842,\n",
       "  'BIPOS_(NN,RB)': 0.10526315789473684,\n",
       "  'BIPOS_(NNP,VBD)': 0.05263157894736842,\n",
       "  'BIPOS_(NNS,VB)': 0.05263157894736842,\n",
       "  'BIPOS_(PRP,VBD)': 0.10526315789473684,\n",
       "  'BIPOS_(RB,PRP)': 0.05263157894736842,\n",
       "  'BIPOS_(VB,NN)': 0.05263157894736842,\n",
       "  'BIPOS_(VBD,MD)': 0.05263157894736842,\n",
       "  'BIPOS_(VBD,NNS)': 0.05263157894736842,\n",
       "  'BIPOS_(VBD,VBN)': 0.05263157894736842,\n",
       "  'BIPOS_(VBG,JJ)': 0.05263157894736842,\n",
       "  'BIPOS_(VBN,NN)': 0.05263157894736842,\n",
       "  \"BI_('d,eating)\": 0.05263157894736842,\n",
       "  'BI_(david,noticed)': 0.05263157894736842,\n",
       "  'BI_(eating,much)': 0.05263157894736842,\n",
       "  'BI_(examined,habits)': 0.05263157894736842,\n",
       "  'BI_(fast,food)': 0.05263157894736842,\n",
       "  'BI_(figure,reason)': 0.05263157894736842,\n",
       "  'BI_(food,lately)': 0.05263157894736842,\n",
       "  'BI_(habits,try)': 0.05263157894736842,\n",
       "  'BI_(he,examined)': 0.05263157894736842,\n",
       "  'BI_(he,realized)': 0.05263157894736842,\n",
       "  'BI_(lot,weight)': 0.05263157894736842,\n",
       "  'BI_(much,fast)': 0.05263157894736842,\n",
       "  'BI_(noticed,put)': 0.05263157894736842,\n",
       "  'BI_(put,lot)': 0.05263157894736842,\n",
       "  \"BI_(realized,'d)\": 0.05263157894736842,\n",
       "  'BI_(reason,he)': 0.05263157894736842,\n",
       "  'BI_(recently,he)': 0.05263157894736842,\n",
       "  'BI_(try,figure)': 0.05263157894736842,\n",
       "  'BI_(weight,recently)': 0.05263157894736842,\n",
       "  'UNIPOS_JJ': 0.05,\n",
       "  'UNIPOS_MD': 0.05,\n",
       "  'UNIPOS_NN': 0.3,\n",
       "  'UNIPOS_NNP': 0.05,\n",
       "  'UNIPOS_NNS': 0.05,\n",
       "  'UNIPOS_PRP': 0.1,\n",
       "  'UNIPOS_RB': 0.1,\n",
       "  'UNIPOS_VB': 0.05,\n",
       "  'UNIPOS_VBD': 0.15,\n",
       "  'UNIPOS_VBG': 0.05,\n",
       "  'UNIPOS_VBN': 0.05,\n",
       "  \"UNI_'d\": 0.05,\n",
       "  'UNI_david': 0.05,\n",
       "  'UNI_eating': 0.05,\n",
       "  'UNI_examined': 0.05,\n",
       "  'UNI_fast': 0.05,\n",
       "  'UNI_figure': 0.05,\n",
       "  'UNI_food': 0.05,\n",
       "  'UNI_habits': 0.05,\n",
       "  'UNI_he': 0.1,\n",
       "  'UNI_lately': 0.05,\n",
       "  'UNI_lot': 0.05,\n",
       "  'UNI_much': 0.05,\n",
       "  'UNI_noticed': 0.05,\n",
       "  'UNI_put': 0.05,\n",
       "  'UNI_realized': 0.05,\n",
       "  'UNI_reason': 0.05,\n",
       "  'UNI_recently': 0.05,\n",
       "  'UNI_try': 0.05,\n",
       "  'UNI_weight': 0.05,\n",
       "  'liwc:negative': 1,\n",
       "  'liwc_achievement': 5.0,\n",
       "  'liwc_biological_processes': 10.0,\n",
       "  'liwc_cognitive_processes': 5.0,\n",
       "  'liwc_common_verbs': 5.0,\n",
       "  'liwc_ingestion': 10.0,\n",
       "  'liwc_motion': 5.0,\n",
       "  'liwc_present_tense': 5.0,\n",
       "  'liwc_quantifiers': 10.0,\n",
       "  'liwc_relativity': 10.0,\n",
       "  'liwc_tentative': 5.0,\n",
       "  'liwc_time': 5.0,\n",
       "  'liwc_total_function_words': 10.0},\n",
       " 'not_joke')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notjoke_feature_tuples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "division_size = int(set_size*4/5)\n",
    "train = joke_feature_tuples[:division_size]+notjoke_feature_tuples[:division_size]\n",
    "dev = joke_feature_tuples[division_size:]+notjoke_feature_tuples[division_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.classify.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.850625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(classifier, dev)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |       n |\n",
      "         |       o |\n",
      "         |       t |\n",
      "         |       _ |\n",
      "         |   j   j |\n",
      "         |   o   o |\n",
      "         |   k   k |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "    joke |<629>171 |\n",
      "not_joke |  68<732>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_only = []\n",
    "labels_only = []\n",
    "for vector, label in dev:\n",
    "    features_only.append(vector)\n",
    "    labels_only.append(label)\n",
    "    \n",
    "\n",
    "predicted_labels = classifier.classify_many(features_only)\n",
    "\n",
    "confusion_matrix = nltk.ConfusionMatrix(labels_only, predicted_labels)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_joke\n",
      "not_joke\n",
      "not_joke\n",
      "not_joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "not_joke\n",
      "joke\n",
      "joke\n",
      "joke\n",
      "joke\n"
     ]
    }
   ],
   "source": [
    "test = \"There was an orange on the tree. Someone was standing under the tree. Here is another sentence. And another one. And here is a fifth.\"\n",
    "test = \"Why is it unknown on how pterodactyls urinate especially during flight?\"\n",
    "test = \"i hate how you can't stop for 7 years. he flings his dibty-out to sitting at a bell and asked the faster ''we started carrying and utteringal at a heary at this first 'could you refresh?' the man comes back to his bought and scared as it's only day, 'i'm walking on by because. the battle runs farted to the entire lives of arm off a body where he refugeed to be from his chair woman as heaven, but she sat up was water.one of it and he finds the ownerhip showing up. this is an old man cated i but start coasination\"\n",
    "tests = [\n",
    "\"what do you tricking without the money?\\\"\\\"there's able together.\\\"there's hairy rounds the hundered, and.a kid unturns the 'other rednecked.\",\n",
    "\n",
    "\"into a bar, i think then. we togl indeader drinking it fur slowly bread with not to has an alleday to tjumble between. after the doors and it wouldn't win? an drants 10 hmph.\",\n",
    "\n",
    "\"what do roly sure it decides her on the bills out of the 7 heevy in lim sorry-kiblive selfripal day and where does pactive a section, and he was grabbing the hospit faster? it's good gare anguagas.  you can't snow likes.son: ih\",\n",
    "\n",
    "\"blon do younmation? \\\"the came at my privator. light.> i asking, \\\"land,\\\" cled day, \\\"why did land why a life wond-r/zep_aggers/k2k came - thatic-coup.immatterbary.one decity crazh of seexic ange s next \\\"ca4\",\n",
    "\n",
    "\"a joke the mouely, he sees on the natted intral remember for supported the looking for a mpc.the things, \\\"yes, i wont he said than why did.\",\n",
    "\n",
    "\"a subjle to a merger... ..   i inkter by cne. \\\"it's usually barely been explaining the foresian for you.one day, s\",\n",
    "\n",
    "\"Whits, says a xear, wold what What the man to hard there heran alinuts to the sistel third the Conks t\",\n",
    "\n",
    "\"Wha love now some he lower told can no head words about it to a brotheretter commite. So you want the\",\n",
    "\n",
    "\"What’s working and a store in are of was got in more by the off the most around bring back as in a st\",\n",
    "\n",
    "\"Where you have he sitting he was a nervose happened. Why can on the cather hold a bor?\",\n",
    "\n",
    "\"Where a monkey can! Why was bikine for the hot do dubbing and all devatrist sex? Because a moment. He\",\n",
    "\n",
    "\"What’s a some from a turn with there? The bull in But I’ll don are ask. He couldn’t better with a ball\",\n",
    "\n",
    "\"i hate how you can't stop for 7 years. he flings his dibty-out to sitting at a bell and asked the faster \\\"\\\"we started carrying and utteringal at a heary at this first \\\"could you refresh?\\\" the man comes back to his bought and scared as it's only day, \\\"i'm walking on by because. the battle runs farted to the entire lives of arm off a body where he refugeed to be from his chair woman as heaven, but she sat up was water.one of it and he finds the ownerhip showing up. this is an old man cated i but start coasination\",\n",
    "\n",
    "\"i hate how you can't st.  \\\"hey, so no, are an expensive breaked the temperature to left, he makes the lead, and the swimming rules in a back of holds. and the inside to great as and old : i just make it off her healfuses. in the wines were logged by a good shop-across and thought more really englishman, thought he heard enough trishung. she giving the hope to be fruit as she was a new debuted pretty drinking and puts it.but they can get it on easy!\",\n",
    "\n",
    "\"i hate how you can't see swet president and asked, \\\"what does it?\\”\",\n",
    "\n",
    "\"i hate how you can't stupid dick?\\\"i\",\n",
    "\n",
    "\"why women need legs.\",\n",
    "\n",
    "\"why women need legse fishing one of his back and starts sboft ingurily.from bay. \\\"you're everyone were cartain.\",\n",
    "\n",
    "\"why women need legsy arms watting the attraction of the guy gets not try. **apain skin on eyem!\",\n",
    "\n",
    "\"why women need legs for a few million loop, and this were holding its time. replied, \\“'jae, so it's jewish tases\",\n",
    "\n",
    "\"what do you think how i fuck us \\\" after god doing jack hole trump are hitting us. say, and to look down!\"\n",
    "\n",
    "]\n",
    "for test in tests:\n",
    "    tokens, tags = normalize(test)\n",
    "    print(classifier.classify({**get_ngrams(tokens), **get_pos(tags), **get_liwc_features(tokens)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try:\n",
    "\n",
    "Word embeddings\n",
    "\n",
    "Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
