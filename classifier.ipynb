{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import math, random\n",
    "import nltk\n",
    "import re\n",
    "import word_category_counter\n",
    "\n",
    "#Get our data\n",
    "#Label data based on rating\n",
    "#Extract features\n",
    "#split between train, dev, and test\n",
    "\n",
    "#Choose classifier \n",
    "#Train classifier (on train data)\n",
    "\n",
    "#Test classifier (on dev data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get our data\n",
    "\n",
    "#standard preprocessing\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "EOJ = 'xeoj'  # end of joke tag\n",
    "\n",
    "#get jokes:\n",
    "PATH=Path('data')\n",
    "\n",
    "files = list(PATH.iterdir())\n",
    "\n",
    "for fname in files:\n",
    "    if \"eddit\" in str(fname):\n",
    "        reddit_dataset = str(fname)\n",
    "    if \"upid\" in str(fname):\n",
    "        stupid_dataset = str(fname)\n",
    "reddit_jokes = json.load(open(reddit_dataset))\n",
    "stupid_jokes = json.load(open(stupid_dataset))\n",
    "\n",
    "#discard reddit jokes with 0 score:\n",
    "rated_jokes = [joke for joke in reddit_jokes if joke['score'] > 0]\n",
    "\n",
    "#regularize to match stupid_jokes:\n",
    "title_body = [joke['title']+' '+joke['body'] for joke in rated_jokes]\n",
    "\n",
    "all_jokes = []\n",
    "for i in range(len(reddit_jokes)):\n",
    "    r_joke = reddit_jokes[i]\n",
    "    #|print(r_joke)\n",
    "    r_joke['rating']=round(math.log(r_joke['score']+random.randrange(1,10))/math.log(10)*5/2, 2)\n",
    "    if r_joke['rating']>5:\n",
    "        r_joke['rating']=5\n",
    "    del r_joke['score'] \n",
    "    r_joke['body'] = r_joke['title']+\" \"+r_joke['body']\n",
    "    del r_joke['title']\n",
    "for s_joke in stupid_jokes:\n",
    "    del s_joke['category']\n",
    "\n",
    "#combine joke sets:\n",
    "combined_jokes = reddit_jokes + stupid_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91745, 106581)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group into funny and notFunny sets:\n",
    "\n",
    "funny_joke_list = []\n",
    "not_funny_joke_list = []\n",
    "for joke in combined_jokes:\n",
    "    if joke[\"rating\"] >= 2.5:\n",
    "        funny_joke_list.append(joke)\n",
    "    else:\n",
    "        not_funny_joke_list.append(joke)\n",
    "        \n",
    "(len(funny_joke_list), len(not_funny_joke_list) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get unigrams and bigrams:\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def normalize(text):\n",
    "    tokenized_text = []\n",
    "    tags = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        intermediate = [word for word in nltk.word_tokenize(sent) if (word not in stopwords) and re.search(r\"\\w\", word)]\n",
    "        for word, pos in nltk.pos_tag(intermediate):\n",
    "            tokenized_text.append(word.lower())\n",
    "            tags.append(pos)\n",
    "    return tokenized_text, tags\n",
    "\n",
    "def get_ngrams(tokens):\n",
    "    unigrams = nltk.FreqDist(tokens)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    \n",
    "    feature_vector = {}\n",
    "    for token, freq in unigrams.items():\n",
    "        feature_vector[\"UNI_%s\" %(token)] = float(freq)/unigrams.N()\n",
    "    for (token1, token2), freq in bigrams.items():\n",
    "        feature_vector[\"BI_(%s,%s)\" %(token1,token2)] = float(freq)/bigrams.N()        \n",
    "    return feature_vector\n",
    "        #\"%s ahhhhh! %s\" %(\"sdflks\", \"sdff\")\n",
    "    \n",
    "def get_pos(tags):\n",
    "    unigrams = nltk.FreqDist(tags)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tags))\n",
    "    \n",
    "    feature_vector = {}\n",
    "    for token, freq in unigrams.items():\n",
    "        feature_vector[\"UNIPOS_%s\" %(token)] = float(freq)/unigrams.N()\n",
    "    for (token1, token2), freq in bigrams.items():\n",
    "        feature_vector[\"BIPOS_(%s,%s)\" %(token1,token2)] = float(freq)/bigrams.N()        \n",
    "    return feature_vector\n",
    "\n",
    "def get_liwc_features(tokens):\n",
    "    \"\"\"\n",
    "    Adds all possible LIWC derived feature\n",
    "\n",
    "    :param words:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text = u\" \".join(tokens)\n",
    "    liwc_cat = ['Total Function Words',\n",
    "                'Total Pronouns',\n",
    "                'Personal Pronouns',\n",
    "                'First Person Singular',\n",
    "                'First Person Plural',\n",
    "                'Second Person',\n",
    "                'Third Person Singular',\n",
    "                'Third Person Plural',\n",
    "                ' Impersonal Pronouns',\n",
    "                'Articles',\n",
    "                'Common Verbs',\n",
    "                'Auxiliary Verbs',\n",
    "                'Past Tense',\n",
    "                'Present Tense',\n",
    "                'Future Tense',\n",
    "                'Adverbs',\n",
    "                'Prepositions',\n",
    "                'Conjunctions',\n",
    "                'Negations',\n",
    "                'Quantifiers',\n",
    "                'Number',\n",
    "                'Swear Words',\n",
    "                'Social Processes',\n",
    "                'Family',\n",
    "                'Friends',\n",
    "                'Humans',\n",
    "                'Affective Processes',\n",
    "                'Positive Emotion',\n",
    "                'Negative Emotion',\n",
    "                'Anxiety',\n",
    "                'Anger',\n",
    "                'Sadness',\n",
    "                'Cognitive Processes',\n",
    "                'Insight',\n",
    "                'Causation',\n",
    "                'Discrepancy',\n",
    "                'Tentative',\n",
    "                'Certainty',\n",
    "                'Inhibition',\n",
    "                'Inclusive',\n",
    "                'Exclusive',\n",
    "                'Perceptual Processes',\n",
    "                'See',\n",
    "                'Hear',\n",
    "                'Feel',\n",
    "                'Biological Processes',\n",
    "                'Body',\n",
    "                'Health',\n",
    "                'Sexual',\n",
    "                'Ingestion',\n",
    "                'Relativity',\n",
    "                'Motion',\n",
    "                'Space',\n",
    "                'Time',\n",
    "                'Work',\n",
    "                'Achievement',\n",
    "                'Leisure',\n",
    "                'Home',\n",
    "                'Money',\n",
    "                'Religion',\n",
    "                'Death',\n",
    "                'Assent',\n",
    "                'Nonfluencies',\n",
    "                'Fillers',\n",
    "                'Total first person',\n",
    "                'Total third person',\n",
    "                'Positive feelings',\n",
    "                'Optimism and energy',\n",
    "                'Communication',\n",
    "                'Other references to people',\n",
    "                'Up',\n",
    "                'Down',\n",
    "                'Occupation',\n",
    "                'School',\n",
    "                'Sports',\n",
    "                'TV',\n",
    "                'Music',\n",
    "                'Metaphysical issues',\n",
    "                'Physical states and functions',\n",
    "                'Sleeping',\n",
    "                'Grooming']\n",
    "\n",
    "    feature_vectors = {}\n",
    "    liwc_scores = word_category_counter.score_text(text)\n",
    "\n",
    "    negative_score = liwc_scores[\"Negative Emotion\"]\n",
    "    positive_score = liwc_scores[\"Positive Emotion\"]\n",
    "\n",
    "    if positive_score > negative_score:\n",
    "        feature_vectors[\"liwc:positive\"] = 1\n",
    "    else:\n",
    "        feature_vectors[\"liwc:negative\"] = 1\n",
    "\n",
    "    for cat in liwc_cat:\n",
    "        if cat in liwc_scores:\n",
    "            label = cat.lower().replace(\" \", \"_\")\n",
    "            feature_vectors[\"liwc_%s\" %label] = liwc_scores[cat]\n",
    "\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "funny_feature_tuples = []\n",
    "set_size= 8000\n",
    "for joke in funny_joke_list[:set_size]:\n",
    "    tokens, tags = normalize(joke[\"body\"])\n",
    "    funny_feature_tuples.append(({**get_ngrams(tokens), **get_pos(tags), **get_liwc_features(tokens)},\"funny\"))\n",
    "    \n",
    "unfunny_feature_tuples = []\n",
    "for joke in not_funny_joke_list[:set_size]:\n",
    "    tokens, tags = normalize(joke[\"body\"])\n",
    "    unfunny_feature_tuples.append(({**get_ngrams(tokens), **get_pos(tags), **get_liwc_features(tokens)},\"unfunny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'BIPOS_(DT,NN)': 0.03278688524590164,\n",
       "  'BIPOS_(DT,NNP)': 0.01639344262295082,\n",
       "  'BIPOS_(JJ,NNP)': 0.04918032786885246,\n",
       "  'BIPOS_(JJ,NNS)': 0.01639344262295082,\n",
       "  'BIPOS_(JJR,JJ)': 0.01639344262295082,\n",
       "  'BIPOS_(MD,VB)': 0.01639344262295082,\n",
       "  'BIPOS_(NN,JJ)': 0.01639344262295082,\n",
       "  'BIPOS_(NN,NN)': 0.03278688524590164,\n",
       "  'BIPOS_(NN,NNP)': 0.04918032786885246,\n",
       "  'BIPOS_(NN,NNS)': 0.01639344262295082,\n",
       "  'BIPOS_(NN,RB)': 0.01639344262295082,\n",
       "  'BIPOS_(NN,VBD)': 0.01639344262295082,\n",
       "  'BIPOS_(NN,VBZ)': 0.03278688524590164,\n",
       "  'BIPOS_(NNP,NN)': 0.03278688524590164,\n",
       "  'BIPOS_(NNP,NNP)': 0.08196721311475409,\n",
       "  'BIPOS_(NNP,NNS)': 0.04918032786885246,\n",
       "  'BIPOS_(NNP,RB)': 0.01639344262295082,\n",
       "  'BIPOS_(NNP,VBZ)': 0.08196721311475409,\n",
       "  'BIPOS_(NNS,MD)': 0.01639344262295082,\n",
       "  'BIPOS_(NNS,NN)': 0.03278688524590164,\n",
       "  'BIPOS_(NNS,NNP)': 0.01639344262295082,\n",
       "  'BIPOS_(NNS,PRP)': 0.01639344262295082,\n",
       "  'BIPOS_(NNS,RB)': 0.01639344262295082,\n",
       "  'BIPOS_(PRP,NNP)': 0.01639344262295082,\n",
       "  'BIPOS_(PRP,VBD)': 0.01639344262295082,\n",
       "  'BIPOS_(PRP,VBZ)': 0.03278688524590164,\n",
       "  'BIPOS_(RB,NNP)': 0.01639344262295082,\n",
       "  'BIPOS_(RB,VBZ)': 0.01639344262295082,\n",
       "  'BIPOS_(VB,JJ)': 0.01639344262295082,\n",
       "  'BIPOS_(VBD,JJ)': 0.01639344262295082,\n",
       "  'BIPOS_(VBD,JJR)': 0.01639344262295082,\n",
       "  'BIPOS_(VBZ,DT)': 0.04918032786885246,\n",
       "  'BIPOS_(VBZ,NN)': 0.04918032786885246,\n",
       "  'BIPOS_(VBZ,NNS)': 0.01639344262295082,\n",
       "  'BIPOS_(VBZ,PRP)': 0.04918032786885246,\n",
       "  'BI_(a,sunday)': 0.01639344262295082,\n",
       "  'BI_(answer,asks)': 0.01639344262295082,\n",
       "  'BI_(answers,he)': 0.01639344262295082,\n",
       "  'BI_(asks,class)': 0.01639344262295082,\n",
       "  'BI_(asks,little)': 0.01639344262295082,\n",
       "  'BI_(bangs,bathroom)': 0.01639344262295082,\n",
       "  'BI_(bathroom,door)': 0.01639344262295082,\n",
       "  'BI_(bathroom,the)': 0.01639344262295082,\n",
       "  'BI_(blurts,he)': 0.01639344262295082,\n",
       "  'BI_(brian,raises)': 0.03278688524590164,\n",
       "  'BI_(christ,still)': 0.01639344262295082,\n",
       "  'BI_(class,where)': 0.01639344262295082,\n",
       "  'BI_(concerned,students)': 0.01639344262295082,\n",
       "  'BI_(confused,jesus)': 0.01639344262295082,\n",
       "  'BI_(dad,gets)': 0.01639344262295082,\n",
       "  'BI_(door,yells)': 0.01639344262295082,\n",
       "  'BI_(every,morning)': 0.01639344262295082,\n",
       "  'BI_(furiously,blurts)': 0.01639344262295082,\n",
       "  'BI_(gets,bangs)': 0.01639344262295082,\n",
       "  'BI_(hand,furiously)': 0.01639344262295082,\n",
       "  'BI_(hand,says)': 0.03278688524590164,\n",
       "  'BI_(he,bathroom)': 0.01639344262295082,\n",
       "  'BI_(he,heart.)': 0.01639344262295082,\n",
       "  'BI_(he,heaven.)': 0.03278688524590164,\n",
       "  'BI_(heart.,little)': 0.01639344262295082,\n",
       "  'BI_(heaven.,a)': 0.01639344262295082,\n",
       "  'BI_(heaven.,susan)': 0.01639344262295082,\n",
       "  'BI_(jesus,asks)': 0.01639344262295082,\n",
       "  'BI_(jesus,christ)': 0.01639344262295082,\n",
       "  'BI_(jesus,today)': 0.01639344262295082,\n",
       "  'BI_(johnny,knows)': 0.01639344262295082,\n",
       "  'BI_(johnny,says)': 0.01639344262295082,\n",
       "  'BI_(johnny,waves)': 0.01639344262295082,\n",
       "  'BI_(knows,well)': 0.01639344262295082,\n",
       "  'BI_(little,confused)': 0.01639344262295082,\n",
       "  'BI_(little,johnny)': 0.04918032786885246,\n",
       "  'BI_(might,little)': 0.01639344262295082,\n",
       "  'BI_(morning,dad)': 0.01639344262295082,\n",
       "  'BI_(raises,hand)': 0.03278688524590164,\n",
       "  'BI_(says,every)': 0.01639344262295082,\n",
       "  'BI_(says,he)': 0.03278688524590164,\n",
       "  'BI_(school,teacher)': 0.01639344262295082,\n",
       "  'BI_(students,might)': 0.01639344262295082,\n",
       "  'BI_(sunday,school)': 0.01639344262295082,\n",
       "  'BI_(surprised,answer)': 0.01639344262295082,\n",
       "  'BI_(susan,answers)': 0.01639344262295082,\n",
       "  'BI_(teacher,concerned)': 0.01639344262295082,\n",
       "  'BI_(teacher,surprised)': 0.01639344262295082,\n",
       "  'BI_(the,teacher)': 0.01639344262295082,\n",
       "  'BI_(today,brian)': 0.01639344262295082,\n",
       "  'BI_(waves,hand)': 0.01639344262295082,\n",
       "  'BI_(well,little)': 0.01639344262295082,\n",
       "  'BI_(where,jesus)': 0.01639344262295082,\n",
       "  'BI_(yells,jesus)': 0.01639344262295082,\n",
       "  'UNIPOS_DT': 0.04838709677419355,\n",
       "  'UNIPOS_JJ': 0.06451612903225806,\n",
       "  'UNIPOS_JJR': 0.016129032258064516,\n",
       "  'UNIPOS_MD': 0.016129032258064516,\n",
       "  'UNIPOS_NN': 0.1774193548387097,\n",
       "  'UNIPOS_NNP': 0.25806451612903225,\n",
       "  'UNIPOS_NNS': 0.0967741935483871,\n",
       "  'UNIPOS_PRP': 0.06451612903225806,\n",
       "  'UNIPOS_RB': 0.04838709677419355,\n",
       "  'UNIPOS_VB': 0.016129032258064516,\n",
       "  'UNIPOS_VBD': 0.03225806451612903,\n",
       "  'UNIPOS_VBZ': 0.16129032258064516,\n",
       "  'UNI_a': 0.016129032258064516,\n",
       "  'UNI_answer': 0.016129032258064516,\n",
       "  'UNI_answers': 0.016129032258064516,\n",
       "  'UNI_asks': 0.03225806451612903,\n",
       "  'UNI_bangs': 0.016129032258064516,\n",
       "  'UNI_bathroom': 0.03225806451612903,\n",
       "  'UNI_blurts': 0.016129032258064516,\n",
       "  'UNI_brian': 0.03225806451612903,\n",
       "  'UNI_christ': 0.016129032258064516,\n",
       "  'UNI_class': 0.016129032258064516,\n",
       "  'UNI_concerned': 0.016129032258064516,\n",
       "  'UNI_confused': 0.016129032258064516,\n",
       "  'UNI_dad': 0.016129032258064516,\n",
       "  'UNI_door': 0.016129032258064516,\n",
       "  'UNI_every': 0.016129032258064516,\n",
       "  'UNI_furiously': 0.016129032258064516,\n",
       "  'UNI_gets': 0.016129032258064516,\n",
       "  'UNI_hand': 0.04838709677419355,\n",
       "  'UNI_he': 0.06451612903225806,\n",
       "  'UNI_heart.': 0.016129032258064516,\n",
       "  'UNI_heaven.': 0.03225806451612903,\n",
       "  'UNI_jesus': 0.04838709677419355,\n",
       "  'UNI_johnny': 0.04838709677419355,\n",
       "  'UNI_knows': 0.016129032258064516,\n",
       "  'UNI_little': 0.06451612903225806,\n",
       "  'UNI_might': 0.016129032258064516,\n",
       "  'UNI_morning': 0.016129032258064516,\n",
       "  'UNI_raises': 0.03225806451612903,\n",
       "  'UNI_says': 0.04838709677419355,\n",
       "  'UNI_school': 0.016129032258064516,\n",
       "  'UNI_still': 0.016129032258064516,\n",
       "  'UNI_students': 0.016129032258064516,\n",
       "  'UNI_sunday': 0.016129032258064516,\n",
       "  'UNI_surprised': 0.016129032258064516,\n",
       "  'UNI_susan': 0.016129032258064516,\n",
       "  'UNI_teacher': 0.03225806451612903,\n",
       "  'UNI_the': 0.016129032258064516,\n",
       "  'UNI_today': 0.016129032258064516,\n",
       "  'UNI_waves': 0.016129032258064516,\n",
       "  'UNI_well': 0.016129032258064516,\n",
       "  'UNI_where': 0.016129032258064516,\n",
       "  'UNI_yells': 0.016129032258064516,\n",
       "  'liwc:positive': 1,\n",
       "  'liwc_adverbs': 3.225806451612903,\n",
       "  'liwc_affective_processes': 3.225806451612903,\n",
       "  'liwc_auxiliary_verbs': 1.6129032258064515,\n",
       "  'liwc_biological_processes': 6.451612903225806,\n",
       "  'liwc_body': 6.451612903225806,\n",
       "  'liwc_certainty': 1.6129032258064515,\n",
       "  'liwc_cognitive_processes': 3.225806451612903,\n",
       "  'liwc_common_verbs': 3.225806451612903,\n",
       "  'liwc_feel': 4.838709677419355,\n",
       "  'liwc_future_tense': 1.6129032258064515,\n",
       "  'liwc_hear': 4.838709677419355,\n",
       "  'liwc_home': 1.6129032258064515,\n",
       "  'liwc_nonfluencies': 1.6129032258064515,\n",
       "  'liwc_perceptual_processes': 9.67741935483871,\n",
       "  'liwc_positive_emotion': 1.6129032258064515,\n",
       "  'liwc_present_tense': 1.6129032258064515,\n",
       "  'liwc_quantifiers': 1.6129032258064515,\n",
       "  'liwc_relativity': 6.451612903225806,\n",
       "  'liwc_social_processes': 4.838709677419355,\n",
       "  'liwc_tentative': 1.6129032258064515,\n",
       "  'liwc_time': 6.451612903225806,\n",
       "  'liwc_total_function_words': 6.451612903225806,\n",
       "  'liwc_work': 3.225806451612903},\n",
       " 'funny')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funny_feature_tuples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "division_size = int(set_size*4/5)\n",
    "train = funny_feature_tuples[:division_size]+unfunny_feature_tuples[:division_size]\n",
    "dev = funny_feature_tuples[division_size:]+unfunny_feature_tuples[division_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.classify.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.554375"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(classifier, dev)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        |       u |\n",
      "        |       n |\n",
      "        |   f   f |\n",
      "        |   u   u |\n",
      "        |   n   n |\n",
      "        |   n   n |\n",
      "        |   y   y |\n",
      "--------+---------+\n",
      "  funny |<823>777 |\n",
      "unfunny | 649<951>|\n",
      "--------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_only = []\n",
    "labels_only = []\n",
    "for vector, label in dev:\n",
    "    features_only.append(vector)\n",
    "    labels_only.append(label)\n",
    "    \n",
    "\n",
    "predicted_labels = classifier.classify_many(features_only)\n",
    "\n",
    "confusion_matrix = nltk.ConfusionMatrix(labels_only, predicted_labels)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try:\n",
    "\n",
    "Word embeddings\n",
    "\n",
    "Binning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
