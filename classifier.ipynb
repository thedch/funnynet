{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beth\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import math, random\n",
    "import nltk\n",
    "import re\n",
    "import word_category_counter\n",
    "import time\n",
    "import word2vec_extractor\n",
    "from sklearn import tree, naive_bayes, svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#W2vecextractor = word2vec_extractor.Word2vecExtractor(\"GoogleNews-vectors-negative300.bin\")\n",
    "\n",
    "#Get our data\n",
    "#Label data based on rating\n",
    "#Extract features\n",
    "#split between train, dev, and test\n",
    "\n",
    "#Choose classifier \n",
    "#Train classifier (on train data)\n",
    "\n",
    "#Test classifier (on dev data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get our data\n",
    "\n",
    "#standard preprocessing\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "EOJ = 'xeoj'  # end of joke tag\n",
    "\n",
    "#get jokes:\n",
    "PATH=Path('data')\n",
    "\n",
    "files = list(PATH.iterdir())\n",
    "\n",
    "for fname in files:\n",
    "    if \"eddit\" in str(fname):\n",
    "        reddit_dataset = str(fname)\n",
    "    if \"upid\" in str(fname):\n",
    "        stupid_dataset = str(fname)\n",
    "reddit_jokes = json.load(open(reddit_dataset))\n",
    "stupid_jokes = json.load(open(stupid_dataset))\n",
    "\n",
    "#discard reddit jokes with 0 score:\n",
    "rated_jokes = [joke for joke in reddit_jokes if joke['score'] > 0]\n",
    "\n",
    "#regularize to match stupid_jokes:\n",
    "title_body = [joke['title']+' '+joke['body'] for joke in rated_jokes]\n",
    "\n",
    "all_jokes = []\n",
    "for i in range(len(reddit_jokes)):\n",
    "    r_joke = reddit_jokes[i]\n",
    "    #|print(r_joke)\n",
    "    r_joke['rating']=round(math.log(r_joke['score']+random.randrange(1,10))/math.log(10)*5/2, 2)\n",
    "    if r_joke['rating']>5:\n",
    "        r_joke['rating']=5\n",
    "    del r_joke['score'] \n",
    "    r_joke['body'] = r_joke['title']+\" \"+r_joke['body']\n",
    "    del r_joke['title']\n",
    "for s_joke in stupid_jokes:\n",
    "    del s_joke['category']\n",
    "\n",
    "#combine joke sets:\n",
    "combined_jokes = reddit_jokes + stupid_jokes\n",
    "\n",
    "random.Random(42).shuffle(combined_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91761, 99694)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group into funny and notFunny sets:\n",
    "\n",
    "funny_joke_list = []\n",
    "not_funny_joke_list = []\n",
    "for joke in combined_jokes:\n",
    "    if joke[\"rating\"] >= 2.5:\n",
    "        funny_joke_list.append(joke)\n",
    "    elif joke[\"rating\"] > 0:\n",
    "        not_funny_joke_list.append(joke)\n",
    "        \n",
    "(len(funny_joke_list), len(not_funny_joke_list) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unigrams and bigrams:\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "liwc_cat_all = ['Total Function Words', 'Total Pronouns', 'Personal Pronouns', \n",
    "            'First Person Singular', 'First Person Plural', 'Second Person', \n",
    "            'Third Person Singular', 'Third Person Plural', ' Impersonal Pronouns', \n",
    "            'Articles', 'Common Verbs', 'Auxiliary Verbs', 'Past Tense', 'Present Tense',  \n",
    "            'Future Tense', 'Adverbs', 'Prepositions', 'Conjunctions', 'Negations', 'Quantifiers',\n",
    "            'Number', 'Swear Words', 'Social Processes', 'Family', 'Friends', 'Humans', \n",
    "            'Affective Processes', 'Positive Emotion', 'Negative Emotion', 'Anxiety', 'Anger',  \n",
    "            'Sadness',  'Cognitive Processes', 'Insight', 'Causation', 'Discrepancy', 'Tentative',\n",
    "            'Certainty', 'Inhibition', 'Inclusive', 'Exclusive', 'Perceptual Processes', 'See', \n",
    "            'Hear', 'Feel', 'Biological Processes', 'Body', 'Health', 'Sexual', 'Ingestion', \n",
    "            'Relativity', 'Motion', 'Space', 'Time', 'Work', 'Achievement', 'Leisure', 'Home', \n",
    "            'Money', 'Religion', 'Death', 'Assent', 'Nonfluencies', 'Fillers', \n",
    "            'Total first person', 'Total third person', 'Positive feelings', 'Optimism and energy',\n",
    "            'Communication', 'Other references to people', 'Up', 'Down', 'Occupation', 'School', \n",
    "            'Sports', 'TV', 'Music', 'Metaphysical issues', 'Physical states and functions', \n",
    "            'Sleeping', 'Grooming']\n",
    "\n",
    "liwc_cat_subset = ['Cognitive Processes','Humans', 'Present Tense','Space','Auxiliary Verbs',\n",
    "                   'Exclusive','Adverbs','Social Processes','Insight','Motion','Quantifiers',\n",
    "                   'Achievement']\n",
    "\n",
    "liwc_cat_binned_subset = ['Death','Anxiety','Social Processes']\n",
    "\n",
    "def normalize(text):\n",
    "    tokenized_text = []\n",
    "    tags = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        intermediate = [word for word in nltk.word_tokenize(sent) \n",
    "                        if (word not in stopwords) and re.search(r\"\\w\", word)]\n",
    "        for word, pos in nltk.pos_tag(intermediate):\n",
    "            tokenized_text.append(word.lower())\n",
    "            tags.append(pos)\n",
    "    return tokenized_text, tags\n",
    "\n",
    "def bin_value(value, cutoff):\n",
    "    return math.ceil( min( math.floor(value), cutoff ) )\n",
    "\n",
    "def get_ngrams(tokens):\n",
    "    unigrams = nltk.FreqDist(tokens)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    \n",
    "    feature_vector = {}\n",
    "    for token, freq in unigrams.items():\n",
    "        feature_vector[\"UNI_%s\" %(token)] = 1#float(freq)/unigrams.N() \n",
    "    for (token1, token2), freq in bigrams.items():\n",
    "        feature_vector[\"BI_(%s,%s)\" %(token1,token2)] = bin_value(float(freq)/bigrams.N() *30, 5)     \n",
    "    return feature_vector\n",
    "        #\"%s ahhhhh! %s\" %(\"sdflks\", \"sdff\")\n",
    "    \n",
    "def get_pos(tags):\n",
    "    unigrams = nltk.FreqDist(tags)\n",
    "    bigrams = nltk.FreqDist(nltk.bigrams(tags))\n",
    "    \n",
    "    feature_vector = {}\n",
    "    for token, freq in unigrams.items():\n",
    "        feature_vector[\"UNIPOS_%s\" %(token)] = bin_value(float(freq)/unigrams.N() *10, 5)\n",
    "    for (token1, token2), freq in bigrams.items():\n",
    "        feature_vector[\"BIPOS_(%s,%s)\" %(token1,token2)] = bin_value(float(freq)/bigrams.N() *30, 5)        \n",
    "    return feature_vector\n",
    "\n",
    "def get_liwc_features(tokens):\n",
    "    \"\"\"\n",
    "    Adds all possible LIWC derived feature\n",
    "\n",
    "    :param words:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text = u\" \".join(tokens)\n",
    "    liwc_cat = list(set(liwc_cat_binned_subset + liwc_cat_subset)) #liwc_cat_all\n",
    "\n",
    "    feature_vectors = {}\n",
    "    liwc_scores = word_category_counter.score_text(text)\n",
    "\n",
    "    for cat in liwc_cat:\n",
    "        if cat in liwc_scores:\n",
    "            label = cat.lower().replace(\" \", \"_\")\n",
    "            feature_vectors[\"liwc_%s\" %label] = bin_value(liwc_scores[cat], 10)\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "def get_word_embeddings(text):\n",
    "    \n",
    "    feature_dict = W2vecextractor.get_doc2vec_feature_dict(text)\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353.945095539093"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "funny_feature_tuples = []\n",
    "set_size= 4000 #int(min(len(funny_joke_list), len(not_funny_joke_list))*4/5)\n",
    "division_size = int(set_size*4/5)\n",
    "\n",
    "# all_tokens = []\n",
    "# for joke in funny_joke_list[:division_size]+not_funny_joke_list[:division_size]:\n",
    "#     tokens, tags = normalize(joke[\"body\"])\n",
    "#     all_tokens+=tokens\n",
    "    \n",
    "# freqDist = nltk.FreqDist(all_tokens)\n",
    "# frequent_words = []\n",
    "# for token, freq in freqDist.items():\n",
    "#     if freq >= 3:\n",
    "#         frequent_words.append(freq)\n",
    "# print(len(frequent_words))\n",
    "# print(len(freqDist.items()))\n",
    "\n",
    "for joke in funny_joke_list[:set_size]:\n",
    "    tokens, tags = normalize(joke[\"body\"])\n",
    "    freq_tokens = [token for token in tokens if token in frequent_words]\n",
    "    funny_feature_tuples.append(({**get_ngrams(tokens), **get_pos(tags), **get_liwc_features(tokens)},\"funny\"))\n",
    "    \n",
    "unfunny_feature_tuples = []\n",
    "for joke in not_funny_joke_list[:set_size]:\n",
    "    tokens, tags = normalize(joke[\"body\"])\n",
    "    freq_tokens = [token for token in tokens if token in frequent_words]\n",
    "    unfunny_feature_tuples.append(({**get_ngrams(tokens), **get_pos(tags), **get_liwc_features(tokens)},\"unfunny\"))\n",
    "    \n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'BIPOS_(DT,NN)': 2,\n",
       "  'BIPOS_(JJR,VBD)': 2,\n",
       "  'BIPOS_(NNS,NNS)': 4,\n",
       "  'BIPOS_(NNS,RB)': 4,\n",
       "  'BIPOS_(RB,VBD)': 2,\n",
       "  'BIPOS_(RB,VBP)': 2,\n",
       "  'BIPOS_(VBD,DT)': 2,\n",
       "  'BIPOS_(VBD,NNS)': 2,\n",
       "  'BIPOS_(VBP,NNS)': 2,\n",
       "  'BIPOS_(VBZ,JJR)': 2,\n",
       "  'BIPOS_(WP,VBZ)': 2,\n",
       "  'BI_(a,seatbelt)': 2,\n",
       "  'BI_(best,jerked)': 2,\n",
       "  'BI_(breasts,inserts)': 2,\n",
       "  'BI_(fits,breasts)': 2,\n",
       "  'BI_(gets,longer)': 2,\n",
       "  'BI_(hole,works)': 2,\n",
       "  'BI_(inserts,neatly)': 2,\n",
       "  'BI_(jerked,a)': 2,\n",
       "  'BI_(longer,pulled)': 2,\n",
       "  'BI_(neatly,hole)': 2,\n",
       "  'BI_(pulled,fits)': 2,\n",
       "  'BI_(what,gets)': 2,\n",
       "  'BI_(works,best)': 2,\n",
       "  'UNIPOS_DT': 0,\n",
       "  'UNIPOS_JJR': 0,\n",
       "  'UNIPOS_NN': 0,\n",
       "  'UNIPOS_NNS': 2,\n",
       "  'UNIPOS_RB': 1,\n",
       "  'UNIPOS_VBD': 1,\n",
       "  'UNIPOS_VBP': 0,\n",
       "  'UNIPOS_VBZ': 0,\n",
       "  'UNIPOS_WP': 0,\n",
       "  'liwc_achievement': 7,\n",
       "  'liwc_present_tense': 7,\n",
       "  'liwc_quantifiers': 7},\n",
       " 'funny')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funny_feature_tuples[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHXMEJCkjhkCktQxQ0CSOvZ/8+lwNeABVLYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5d110dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_name = \"UNIPOS_NN\"\n",
    "tuples = [feature_tuple[0] for feature_tuple \n",
    "          in funny_feature_tuples + unfunny_feature_tuples\n",
    "          if feature_name in feature_tuple[0]]\n",
    "\n",
    "values = [t[feature_name]*10 for t in tuples]\n",
    "plt.hist(values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "division_size = int(set_size*4/5)\n",
    "train = funny_feature_tuples[:division_size]+unfunny_feature_tuples[:division_size]\n",
    "dev = funny_feature_tuples[division_size:set_size]+unfunny_feature_tuples[division_size:set_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = nltk.classify.NaiveBayesClassifier.train(train)\n",
    "# classifier.most_informative_features(100)\n",
    "classifier = nltk.classify.scikitlearn\\\n",
    "           .SklearnClassifier(naive_bayes.BernoulliNB(binarize=False)).train(train)\n",
    "    \n",
    "#classifier = nltk.classify.scikitlearn.SklearnClassifier(svm.SVC()).train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55625"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = nltk.classify.accuracy(classifier, dev)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        |       u |\n",
      "        |       n |\n",
      "        |   f   f |\n",
      "        |   u   u |\n",
      "        |   n   n |\n",
      "        |   n   n |\n",
      "        |   y   y |\n",
      "--------+---------+\n",
      "  funny |<407>393 |\n",
      "unfunny | 317<483>|\n",
      "--------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_only = []\n",
    "labels_only = []\n",
    "for vector, label in dev:\n",
    "    features_only.append(vector)\n",
    "    labels_only.append(label)\n",
    "    \n",
    "\n",
    "predicted_labels = classifier.classify_many(features_only)\n",
    "\n",
    "confusion_matrix = nltk.ConfusionMatrix(labels_only, predicted_labels)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to try:\n",
    "\n",
    "Word embeddings\n",
    "\n",
    "Binning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
